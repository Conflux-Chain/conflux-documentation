"use strict";(self.webpackChunkconflux_docs=self.webpackChunkconflux_docs||[]).push([[1731],{78704:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>c,toc:()=>l});var i=t(85893),o=t(11151);const s={displayed_sidebar:"generalSidebar"},r="Conflux \u5171\u8bc6\u5c42\u7684\u8bbe\u8ba1\u4e0e\u5b9e\u73b0",c={id:"general/build/node-development/consensus-design",title:"Conflux \u5171\u8bc6\u5c42\u7684\u8bbe\u8ba1\u4e0e\u5b9e\u73b0",description:"Conflux \u7684\u5171\u8bc6\u5c42\u5904\u7406\u4ece\u540c\u6b65\u5c42\u63a5\u6536\u5230\u7684\u6240\u6709\u533a\u5757\uff0c\u6839\u636e Conflux GHAST \u5171\u8bc6\u7b97\u6cd5\u4ea7\u751f\u533a\u5757\u7684\u5b8c\u6574\u987a\u5e8f\uff0c\u5e76\u8c03\u7528\u5e95\u5c42\u7684\u4ea4\u6613\u6267\u884c\u5f15\u64ce\u4ee5\u6309\u786e\u5b9a\u7684\u987a\u5e8f\u8fd0\u884c\u4ea4\u6613\u3002 \u5b83\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u4fe1\u606f\uff0c\u4ee5\u534f\u52a9 \u533a\u5757\u751f\u6210\u5668 \u51c6\u5907\u65b0\u533a\u5757\u7684\u9aa8\u67b6\u3002 \u5b83\u8fd8\u901a\u77e5 \u4ea4\u6613\u6c60(transaction pool) \u5df2\u5904\u7406\u597d\u7684\u4ea4\u6613\uff0c\u4ee5\u4fbf\u4ea4\u6613\u6c60\u53ef\u4ee5\u505a\u51fa\u66f4\u597d\u7684\u4ea4\u6613\u9009\u62e9\u51b3\u7b56\u3002",source:"@site/i18n/zh/docusaurus-plugin-content-docs/current/general/build/node-development/consensus-design.md",sourceDirName:"general/build/node-development",slug:"/general/build/node-development/consensus-design",permalink:"/zh-CN/docs/general/build/node-development/consensus-design",draft:!1,unlisted:!1,editUrl:"https://crowdin.com/project/conflux/zh-CN",tags:[],version:"current",frontMatter:{displayed_sidebar:"generalSidebar"},sidebar:"generalSidebar",previous:{title:"Node Development",permalink:"/zh-CN/docs/category/node-development"},next:{title:"Conflux\u7684\u53ef\u9760\u6027\u6d4b\u8bd5\u5de5\u5177",permalink:"/zh-CN/docs/general/build/node-development/rigorous-testing"}},a={},l=[{value:"\u8bbe\u8ba1\u76ee\u6807",id:"\u8bbe\u8ba1\u76ee\u6807",level:2},{value:"\u7ed3\u6784\u4e0e\u7ec4\u6210\u90e8\u5206\u3002",id:"\u7ed3\u6784\u4e0e\u7ec4\u6210\u90e8\u5206",level:2},{value:"ConsensusGraph",id:"consensusgraph",level:3},{value:"ConsensusGraphInner",id:"consensusgraphinner",level:3},{value:"ConsensusNewBlockHandler",id:"consensusnewblockhandler",level:3},{value:"ConsensusExecutor",id:"consensusexecutor",level:3},{value:"ConfirmationMeter",id:"confirmationmeter",level:3},{value:"AnticoneCache and PastsetCache",id:"anticonecache-and-pastsetcache",level:3},{value:"Important Algorithmic Mechanisms",id:"important-algorithmic-mechanisms",level:2},{value:"Pivot Chain and Total Order",id:"pivot-chain-and-total-order",level:3},{value:"Timer Chain",id:"timer-chain",level:3},{value:"Weight Maintenance with Link-Cut Tree",id:"weight-maintenance-with-link-cut-tree",level:3},{value:"Adaptive Weight",id:"adaptive-weight",level:3},{value:"Partial Invalid",id:"partial-invalid",level:3},{value:"Anticone, Past View, and Ledger View",id:"anticone-past-view-and-ledger-view",level:3},{value:"Check Correct Parent",id:"check-correct-parent",level:3},{value:"Fallback Brute Force Methods",id:"fallback-brute-force-methods",level:3},{value:"Force Confirmation",id:"force-confirmation",level:3},{value:"Era",id:"era",level:3},{value:"Checkpoint",id:"checkpoint",level:3},{value:"Deferred Execution",id:"deferred-execution",level:3},{value:"Block Reward Calculation",id:"block-reward-calculation",level:3},{value:"Blaming Mechanism",id:"blaming-mechanism",level:3},{value:"Multi-Thread Design",id:"multi-thread-design",level:2},{value:"Consensus Worker",id:"consensus-worker",level:3},{value:"Consensus Execution Worker",id:"consensus-execution-worker",level:3},{value:"Key Assumptions, Invariants, and Rules",id:"key-assumptions-invariants-and-rules",level:2}];function h(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",strong:"strong",...(0,o.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"conflux-\u5171\u8bc6\u5c42\u7684\u8bbe\u8ba1\u4e0e\u5b9e\u73b0",children:"Conflux \u5171\u8bc6\u5c42\u7684\u8bbe\u8ba1\u4e0e\u5b9e\u73b0"}),"\n",(0,i.jsxs)(n.p,{children:["Conflux \u7684\u5171\u8bc6\u5c42\u5904\u7406\u4ece\u540c\u6b65\u5c42\u63a5\u6536\u5230\u7684\u6240\u6709\u533a\u5757\uff0c\u6839\u636e Conflux GHAST \u5171\u8bc6\u7b97\u6cd5\u4ea7\u751f\u533a\u5757\u7684\u5b8c\u6574\u987a\u5e8f\uff0c\u5e76\u8c03\u7528\u5e95\u5c42\u7684\u4ea4\u6613\u6267\u884c\u5f15\u64ce\u4ee5\u6309\u786e\u5b9a\u7684\u987a\u5e8f\u8fd0\u884c\u4ea4\u6613\u3002 \u5b83\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u4fe1\u606f\uff0c\u4ee5\u534f\u52a9 ",(0,i.jsx)(n.strong,{children:"\u533a\u5757\u751f\u6210\u5668"})," \u51c6\u5907\u65b0\u533a\u5757\u7684\u9aa8\u67b6\u3002 \u5b83\u8fd8\u901a\u77e5 ",(0,i.jsx)(n.strong,{children:"\u4ea4\u6613\u6c60(transaction pool)"})," \u5df2\u5904\u7406\u597d\u7684\u4ea4\u6613\uff0c\u4ee5\u4fbf\u4ea4\u6613\u6c60\u53ef\u4ee5\u505a\u51fa\u66f4\u597d\u7684\u4ea4\u6613\u9009\u62e9\u51b3\u7b56\u3002"]}),"\n",(0,i.jsxs)(n.p,{children:["\u672c\u6587\u6863\u65e8\u5728\u4e3a\u60f3\u8981\u4e86\u89e3 Conflux \u5171\u8bc6\u5c42\uff08\u4f4d\u4e8e\u76ee\u5f55core/src/consensus\u4e2d\uff09\u7684 Rust \u5b9e\u73b0\u7684\u8bfb\u8005\u63d0\u4f9b\u9ad8\u7ea7\u6982\u8ff0\u3002 \u5bf9\u4e8e\u66f4\u591a\u7684\u5b9e\u73b0\u7ec6\u8282\uff0c\u8bf7\u67e5\u770b\u4ee3\u7801\u4e2d\u7684\u5185\u8054\u6ce8\u91ca\u3002 \u5bf9\u4e8e Conflux \u5171\u8bc6\u7b97\u6cd5\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 Conflux \u534f\u8bae\u89c4\u8303\u548c Conflux \u8bba\u6587\uff08",(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/1805.03870%EF%BC%89%E3%80%82",children:"https://arxiv.org/abs/1805.03870\uff09\u3002"})]}),"\n",(0,i.jsx)(n.h2,{id:"\u8bbe\u8ba1\u76ee\u6807",children:"\u8bbe\u8ba1\u76ee\u6807"}),"\n",(0,i.jsx)(n.p,{children:"\u5171\u8bc6\u5c42\u6709\u4ee5\u4e0b\u8bbe\u8ba1\u76ee\u6807\u3002"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"\u5728\u540e\u53f0\u6309\u7167\u4e00\u81f4\u7684\u5171\u8bc6\u7b97\u6cd5\u5904\u7406\u65b0\u7684\u533a\u5757"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"\u6211\u4eec\u5e0c\u671b\u6700\u5c0f\u5316\u5171\u8bc6\u56fe\u4e2d\u6bcf\u4e2a\u5757\u7684\u5185\u5b58\u4f7f\u7528\u3002\n\u5373\u4f7f\u6709\u68c0\u67e5\u70b9\u673a\u5236\uff0c\u5728\u6b63\u5e38\u60c5\u51b5\u4e0b\u56fe\u4e2d\u4f1a\u5305\u542b 300K-500K \u4e2a\u5757\uff0c\u5728\u9762\u5bf9\u5b58\u6d3b\u6027\u653b\u51fb\u65f6\u53ef\u80fd\u4f1a\u8d85\u8fc7 1M \u4e2a\u5757\u3002 \u8fd9\u53ef\u80fd\u4f1a\u7ed9\u5185\u5b58\u5e26\u6765\u538b\u529b\u3002"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"\u6211\u4eec\u60f3\u8981\u5feb\u901f\u5904\u7406\u6bcf\u4e2a\u533a\u5757\u3002 \u56e0\u4e3a\u5168\u8282\u70b9/\u5f52\u6863\u8282\u70b9\u5728\u4ece\u96f6\u5f00\u59cb\u540c\u6b65\u7f51\u7edc\u65f6\u5fc5\u987b\u5904\u7406\u4ece_\u521d\u59cb\u521b\u4e16\u533a\u5757_\u5f00\u59cb\u7684\u4e4b\u540e\u6bcf\u4e00\u4e2a\u533a\u5757\uff0c\u56e0\u6b64\u5feb\u901f\u5904\u7406\u533a\u5757\u5bf9\u4e8e\u7f29\u77ed\u6240\u9700\u65f6\u95f4\u662f\u975e\u5e38\u91cd\u8981\u7684\u3002"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"\u9762\u5bf9\u6f5c\u5728\u653b\u51fb\u65f6\u5177\u6709\u7a33\u5065\u6027\u3002 \u6076\u610f\u653b\u51fb\u8005\u53ef\u80fd\u4f1a\u5728TreeGraph\u7684\u4efb\u610f\u4f4d\u7f6e\u751f\u6210\u6076\u610f\u533a\u5757\u3002"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"\u7ed3\u6784\u4e0e\u7ec4\u6210\u90e8\u5206",children:"\u7ed3\u6784\u4e0e\u7ec4\u6210\u90e8\u5206\u3002"}),"\n",(0,i.jsx)(n.h3,{id:"consensusgraph",children:"ConsensusGraph"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"ConsensusGraph"})," (core/src/consensus/mod.rs) is the main struct of the\nconsensus layer. The synchronization layer constructs ",(0,i.jsx)(n.code,{children:"ConsensusGraph"})," with a\n",(0,i.jsx)(n.code,{children:"BlockDataManager"})," which stores all block metadata information on disk.\n",(0,i.jsx)(n.code,{children:"ConsensusGraph::on_new_block()"})," is the key function to send new blocks to the\n",(0,i.jsx)(n.code,{children:"ConsensusGraph"})," struct to process. It also provides a set of public functions\nto query the status of blocks/transactions. This should be the main interface\nwith which other components interact."]}),"\n",(0,i.jsx)(n.h3,{id:"consensusgraphinner",children:"ConsensusGraphInner"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"ConsensusGraphInner"})," (core/src/consensus/consensus_inner/mod.rs) is the inner\nstructure of ",(0,i.jsx)(n.code,{children:"ConsensusGraph"}),". ",(0,i.jsx)(n.code,{children:"ConsensusGraph::on_new_block()"})," acquires the\nwrite lock of the inner struct at the start of the function. The rest are\nquery functions that only acquire read locks."]}),"\n",(0,i.jsxs)(n.p,{children:["The internal structure of ",(0,i.jsx)(n.code,{children:"ConsensusGraphInner"})," is fairly complicated.\nGenerally speaking, it maintains two kinds of information. The first kind of\ninformation is the state of the whole TreeGraph, i.e., the current ",(0,i.jsx)(n.em,{children:"pivot\nchain"}),", ",(0,i.jsx)(n.em,{children:"timer chain"}),", ",(0,i.jsx)(n.em,{children:"difficulty"}),", etc.. The second kind of information is\nthe state of each block (i.e., ",(0,i.jsx)(n.code,{children:"ConsensusGraphNode"})," struct for each block).\nEach block corresponds to a ",(0,i.jsx)(n.code,{children:"ConsensusGraphNode"})," struct for its information.\nWhen it first enters ",(0,i.jsx)(n.code,{children:"ConsensusGraphInner"}),", it will be inserted into\n",(0,i.jsx)(n.code,{children:"ConsensusGraphInner::arena : Slab<ConsensusGraphNode>"}),". The index in the\nslab will become the arena index of the block in ",(0,i.jsx)(n.code,{children:"ConsensusGraphInner"}),". We use\nthe arena index to represent a block internally instead of ",(0,i.jsx)(n.code,{children:"H256"})," because it is\nmuch cheaper. We will refer back to the fields in ",(0,i.jsx)(n.code,{children:"ConsensusGraphInner"})," and\n",(0,i.jsx)(n.code,{children:"ConsensusGraphNode"})," when we talk about algorithm mechanism and their\nimplementations."]}),"\n",(0,i.jsx)(n.h3,{id:"consensusnewblockhandler",children:"ConsensusNewBlockHandler"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"ConsensusNewBlockHandler"}),"\n(core/src/consensus/consensus_inner/consensus_new_block_handler.rs) contains a\nset of routines for processing a new block. In theory, this code could be part\nof ",(0,i.jsx)(n.code,{children:"ConsensusGraphInner"})," because it mostly manipulates the inner struct.\nHowever, these routines are all subroutine of the ",(0,i.jsx)(n.code,{children:"on_new_block()"})," and the\nconsensus_inner/mod.rs is already very complicated. We therefore decided to put\nthem into a separate file."]}),"\n",(0,i.jsx)(n.h3,{id:"consensusexecutor",children:"ConsensusExecutor"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"ConsensusExecutor"})," (core/src/consensus/consensus_inner/consensus_executor.rs)\nis the interface struct for the standalone transaction execution thread.\n",(0,i.jsx)(n.code,{children:"ConsensusExecutor::enqueue_epoch()"})," allows other threads to send an execution\ntask to execute the epoch of a given pivot chain block asynchronously. Once the\ncomputation finishes, the resulting state root will be stored into\n",(0,i.jsx)(n.code,{children:"BlockDataManager"}),". Other threads can call\n",(0,i.jsx)(n.code,{children:"ConsensusExecutor::wait_for_result()"})," to wait for the execution of an epoch if\ndesired. In the current implementation, ",(0,i.jsx)(n.code,{children:"ConsensusExecutor"})," also contains the\nroutines for the calculation for block rewards, including\n",(0,i.jsx)(n.code,{children:"get_reward_execution_info()"})," and its subroutines."]}),"\n",(0,i.jsx)(n.h3,{id:"confirmationmeter",children:"ConfirmationMeter"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"ConfirmationMeter"})," (core/src/consensus/consensus_inner/confirmation_meter.rs)\nconservatively calculates the confirmation risk of each pivot chain block. Its\nresult will be useful for the storage layer to determine when it is ",(0,i.jsx)(n.em,{children:"safe"})," to\ndiscard old snapshots. It can also be used to serve RPC queries about block\nconfirmation if we decide to provide such RPC."]}),"\n",(0,i.jsx)(n.h3,{id:"anticonecache-and-pastsetcache",children:"AnticoneCache and PastsetCache"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"AnticoneCache"})," (core/src/consensus/anticone_cache.rs) and ",(0,i.jsx)(n.code,{children:"PastsetCache"}),"\n(core/src/consensus/pastset_cache.rs) are two structs that implement customized\ncaches for data structures in ",(0,i.jsx)(n.code,{children:"ConsensusGraphInner"}),". In the implementation of\nthe inner struct, we need to calculate and store the anticone set and the past\nset of each block. However, it is not possible to store all of these sets in\nmemory. We therefore implement cache style data structures to store sets for\nrecently inserted/accessed blocks. If an anticone/past set is not found in the\ncache, we will recalculate the set in the current inner struct implementation."]}),"\n",(0,i.jsx)(n.h2,{id:"important-algorithmic-mechanisms",children:"Important Algorithmic Mechanisms"}),"\n",(0,i.jsx)(n.p,{children:"There are several important algorithmic mechanisms in the Conflux Consensus\nLayer. Here we will talk about them from the implementation aspect. See XXX for\nthe algorithmic reasoning behind them."}),"\n",(0,i.jsx)(n.h3,{id:"pivot-chain-and-total-order",children:"Pivot Chain and Total Order"}),"\n",(0,i.jsx)(n.p,{children:"The basic idea of the Conflux consensus algorithm is to first make everyone\nagree on a pivot chain. It then expands the total order from the pivot chain to\ncover all blocks with a topological sort. As long as the pivot chain does not\nchange/reorg, the total order of blocks will stay the same, so does the derived\norder of transactions."}),"\n",(0,i.jsx)(n.p,{children:"Comparing with Bitcoin/Ethereum, the consensus in Conflux has two key\ndifferences:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"almost every block"})," will go into the total order, not just the agreed pivot\nchain."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["The transaction validity and the block validity are ",(0,i.jsx)(n.em,{children:"independent"}),". For example, a\ntransaction is invalid if it was included before or it cannot carry out due to\ninsufficient balance. Such invalid transactions will become noop during the\nexecution. However, ",(0,i.jsx)(n.em,{children:"unlike Bitcoin and Ethereum blocks containing such\ntransactions will not become invalid"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["In ",(0,i.jsx)(n.code,{children:"ConsensusGraphInner"}),", the arena index of the current pivot chain blocks are\nstored in order in the ",(0,i.jsx)(n.code,{children:"pivot_chain[]"})," vector. To maintain it, we calculate the\nlowest common ancestor (LCA) between the newly inserted block and the current best\nblock following the GHAST rule. If the fork corresponding to the newly inserted\nblock for the LCA ended up to be heavier, we will update the ",(0,i.jsx)(n.code,{children:"pivot_chain[]"})," from\nthe forked point."]}),"\n",(0,i.jsx)(n.h3,{id:"timer-chain",children:"Timer Chain"}),"\n",(0,i.jsxs)(n.p,{children:["Blocks whose PoW quality is ",(0,i.jsx)(n.code,{children:"timer_chain_difficulty_ratio"})," times higher than the target\ndifficulty are ",(0,i.jsx)(n.em,{children:"timer blocks"}),". The ",(0,i.jsx)(n.code,{children:"is_timer"})," field of the block will be set to\nTrue. The consensus algorithm then finds the longest timer block chain (more\naccurately, with greatest accumulated difficulty) similar to the Bitcoin\nconsensus algorithm of finding the longest chain. The arena index of this\nlongest timer chain will be stored into ",(0,i.jsx)(n.code,{children:"timer_chain[]"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["The rationale of the timer chain is to provide a coarse-grained measurement of\ntime that cannot be influenced by a malicious attacker. Because timer blocks\nare rare and generated slowly (if ",(0,i.jsx)(n.code,{children:"timer_chain_difficulty_ratio"})," is properly\nhigh), a malicious attacker cannot prevent the growth of the timer chain unless\nit has the majority of the computation power. Therefore how many timer chain\nblocks appear in the past set of a block is a good indication about the latest\npossible generation time of the block. We compute this value for each block and\nstore it in ",(0,i.jsx)(n.code,{children:"timer_chain_height"})," field of the block."]}),"\n",(0,i.jsx)(n.h3,{id:"weight-maintenance-with-link-cut-tree",children:"Weight Maintenance with Link-Cut Tree"}),"\n",(0,i.jsxs)(n.p,{children:["To effectively maintain the pivot chain, we need to query the total weight of a\nsubtree. Conflux uses a Link-Cut Tree data structure to maintain the subtree\nweights in O(log n). The Link-Cut Tree can also calculate the LCA of any two nodes\nin the TreeGraph in O(log n). The ",(0,i.jsx)(n.code,{children:"weight_tree"})," field in ",(0,i.jsx)(n.code,{children:"ConsensusGraphInner"}),"\nis the link-cut tree that stores the subtree weight of every node. Note that\nthe implementation of the Link-Cut Tree is in the utils/link-cut-tree\ndirectory."]}),"\n",(0,i.jsx)(n.h3,{id:"adaptive-weight",children:"Adaptive Weight"}),"\n",(0,i.jsxs)(n.p,{children:["If the TreeGraph is under a liveness attack, it may fail to converge under one\nblock for a while. To handle this situation, the GHAST algorithm idea is to\nstart to generate adaptive blocks, i.e., blocks whose weights are redistributed\nsignificantly so that there will be many zero weight blocks with a rare set of\nvery heavy blocks. Specifically, if the PoW quality of an adaptive block is\n",(0,i.jsx)(n.code,{children:"adaptive_heavy_block_ratio"})," times of the target difficulty, the block\nwill have a weight of ",(0,i.jsx)(n.code,{children:"adaptive_heavy_block_ratio"}),"; otherwise, the block will\nhave a weight of zero. This effectively slows down the confirmation\ntemporarily but will ensure the consensus progress."]}),"\n",(0,i.jsxs)(n.p,{children:["Because adaptive weight is a mechanism to defend against rare liveness attacks,\nit should not be turned on during the normal scenario. A new block is adaptive\nonly if: 1) one of its ancestor blocks is still not the dominant subtree\ncomparing to its siblings, and 2) a significantly long period of time has passed\nbetween the generation of that ancestor block and the new block (i.e., the\ndifference of ",(0,i.jsx)(n.code,{children:"timer_chain_height"})," is sufficiently large). ",(0,i.jsx)(n.code,{children:"ConsensusGraphInner::adaptive_weight()"}),"\nand its subroutines implement the algorithm to determine whether a block is\nadaptive or not. Note that the implementation uses another link-cut-tree\n",(0,i.jsx)(n.code,{children:"adaptive_tree"})," as a helper. Please see the inlined comments for the\nimplementation details."]}),"\n",(0,i.jsx)(n.h3,{id:"partial-invalid",children:"Partial Invalid"}),"\n",(0,i.jsx)(n.p,{children:"Note that the past set of a new block denotes all the blocks that the generator\nof the new block observes at the generation time. Therefore, from the past set\nof a new block, other full nodes could determine whether it chooses the correct\nparent block and whether it should be adaptive or not."}),"\n",(0,i.jsxs)(n.p,{children:["The Conflux consensus algorithm defines those blocks who choose incorrect\nparents or fill in incorrect adaptive status as ",(0,i.jsx)(n.em,{children:"partial invalid blocks"}),". For a\npartial invalid block, the ",(0,i.jsx)(n.code,{children:"partial_invalid"})," field will be set to True. The\nalgorithm requires the partial invalid blocks being treated differently from\nthe normal blocks in three ways:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["All honest nodes will not reference directly or indirectly partial invalid\nblocks until a significant period of time. This time period is measured with\nthe ",(0,i.jsx)(n.code,{children:"timer_chain_height"})," and the difference has to be more than\n",(0,i.jsx)(n.code,{children:"timer_chain_beta"}),". Yes, it means that if another otherwise perfectly fine\nblock referencing the partial invalid block, both of these two blocks will not\nbe referenced for a while."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Partial invalid blocks will have no block reward. They are extremely\nunlikely to get any reward anyway because of their large anticone set due to\nthe first rule."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Partial invalid blocks are excluded from the timer chain consideration."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["To implement the first rule, the ",(0,i.jsx)(n.code,{children:"on_new_block()"})," routine in\n",(0,i.jsx)(n.code,{children:"ConsensusNewBlockHandler"})," is separated into two subroutine\n",(0,i.jsx)(n.code,{children:"preactivate_block()"})," and ",(0,i.jsx)(n.code,{children:"activate_block()"}),". ",(0,i.jsx)(n.code,{children:"preactivate_block()"})," compute and\ndetermine whether a block is partial invalid or not, while ",(0,i.jsx)(n.code,{children:"activate_block()"}),"\nfully integrate a block into the consensus graph inner data structures. For\nevery new block, the field ",(0,i.jsx)(n.code,{children:"active_cnt"})," tracks how many inactive blocks it\nreferences. A block is inactive if it references directly or indirectly a\npartial invalid block. ",(0,i.jsx)(n.code,{children:"activate_block()"})," will be called on a block only when\n",(0,i.jsx)(n.code,{children:"active_cnt"})," of the block becomes zero. The field ",(0,i.jsx)(n.code,{children:"activated"})," denotes whether a\nblock is active or not. For partially invalid blocks, their activation will be\ndelayed till the current timer chain height of the ledger is ",(0,i.jsx)(n.code,{children:"timer_chain_beta"}),"\nhigher than the invalid block. Newly generated blocks will not reference any\ninactive blocks, i.e., these inactive blocks are treated as if they were not in\nthe TreeGraph."]}),"\n",(0,i.jsx)(n.h3,{id:"anticone-past-view-and-ledger-view",children:"Anticone, Past View, and Ledger View"}),"\n",(0,i.jsxs)(n.p,{children:["In order to check the partial invalid status of each block, we need to operate\nunder the ",(0,i.jsx)(n.em,{children:"past view"})," of the block to determine its correct parent and its\nadaptivity. This is different from the current state of the TreeGraph or we\ncall it the ",(0,i.jsx)(n.em,{children:"ledger view"}),", i.e., all blocks in the anticone and the future set\nof the block are excluded. Because we process blocks in topological order, the\nfuture set of a new block is empty. We therefore need to eliminate all anticone\nblocks only."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"compute_and_update_anticone()"})," in ",(0,i.jsx)(n.code,{children:"ConsensusNewBlockHandler"})," computes the\nanticone set of a new block. Note that because the anticone set may be very\nlarge, we have two implementation level optimizations. First, we represent the\nanticone set as a set of barrier nodes in the TreeGraph, i.e., a set of\nsubtrees where each block in the subtrees is in the anticone set. Second, we\nwill maintain the anticone set of the recently accessed/inserted blocks\nonly. When checking whether a block is valid in its past view or not (e.g., in\n",(0,i.jsx)(n.code,{children:"adaptive_weight()"})," and in ",(0,i.jsx)(n.code,{children:"check_correct_parent()"}),"), we first cut all barrier\nsubtrees from the link-cut weight trees accordingly to get the state of the\npast view. After the computation, we restore these anticone subtrees."]}),"\n",(0,i.jsx)(n.h3,{id:"check-correct-parent",children:"Check Correct Parent"}),"\n",(0,i.jsxs)(n.p,{children:["To check whether a new block chooses a correct parent block or not, we first\ncompute the set of blocks inside the epoch of the new block assuming that\nthe new block is on the pivot chain. We store this set to the field\n",(0,i.jsx)(n.code,{children:"blockset_in_own_view_of_epoch"}),". We then iterate over every candidate block in\nthis set to make sure that the chosen parent block is better than it.\nSpecifically, we find out the two fork blocks of the candidate block and the\nparent block from their LCA and make sure that the fork of the parent is\nheavier. This logic is implemented in ",(0,i.jsx)(n.code,{children:"check_correct_parent()"})," in\n",(0,i.jsx)(n.code,{children:"ConsensusNewBlockHandler"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["Note that ",(0,i.jsx)(n.code,{children:"blockset_in_own_view_of_epoch"})," may become too large to hold\nconsistently in memory as well. Especially if a malicious attacker tries to\ngenerate invalid blocks to blow up this set. The current implementation will\nonly periodically clear the set and only keep the sets for pivot chain blocks.\nNote that for pivot chain blocks, this set will also be used during the\ntransaction execution."]}),"\n",(0,i.jsx)(n.h3,{id:"fallback-brute-force-methods",children:"Fallback Brute Force Methods"}),"\n",(0,i.jsxs)(n.p,{children:["There are situations where the anticone barrier set is too large if a malicious\nattacker tries to launch a performance attack on Conflux. This will make the\ndefault strategy worse than O(n) because there is a factor of O(log n) for each\nblock in the barrier set when we do the link-cut tree chopping. To this end, we\nimplemented a brute force routine ",(0,i.jsx)(n.code,{children:"compute_subtree_weights()"})," to compute the\nsubtree weights of each block in a past view for O(n). We also implement\n",(0,i.jsx)(n.code,{children:"check_correct_parent_brutal()"})," and ",(0,i.jsx)(n.code,{children:"adaptive_weight_impl_brutal()"})," to use the\nbrute-force computed subtree weight to do the checking instead."]}),"\n",(0,i.jsx)(n.h3,{id:"force-confirmation",children:"Force Confirmation"}),"\n",(0,i.jsxs)(n.p,{children:["The Conflux consensus algorithm will ",(0,i.jsx)(n.em,{children:"force confirm"})," a block if 1) there are\n",(0,i.jsx)(n.code,{children:"timer_chain_beta"})," consecutive timer chain blocks under the subtree of the\nblock and 2) afterward there are at least ",(0,i.jsx)(n.code,{children:"timer_chain_beta"})," timer chain blocks\nfollowing (not required in the subtree though). Force confirmation means that\nnew blocks should follow this block as their ancestor no matter what, ignoring\nsubtree weights. Though extremely unlikely a force confirmed block will have\nlesser weights than its siblings."]}),"\n",(0,i.jsx)(n.p,{children:"The force confirmation mechanism is to enable checkpoint, which we will\ndescribe later. It is based on the rationale that:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Reverting a ",(0,i.jsx)(n.code,{children:"timer_chain_beta"})," length timer chain is impossible."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Therefore force confirmed block will always move along the pivot chain, not\ndrifting between its siblings."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["We compute the accumulative LCA of the last ",(0,i.jsx)(n.code,{children:"timer_chain_beta"})," timer chain\nblocks and store it at the ",(0,i.jsx)(n.code,{children:"timer_chain_accumulative_lca[]"})," field. This vector\nis ",(0,i.jsx)(n.code,{children:"timer_chain_beta"})," shorter than ",(0,i.jsx)(n.code,{children:"timer_chain[]"})," because the force confirm\nneeds at least ",(0,i.jsx)(n.code,{children:"timer_chain_beta"})," timer chain block trailing, so their LCAs do\nnot matter. ",(0,i.jsx)(n.code,{children:"check_correct_parent()"})," and ",(0,i.jsx)(n.code,{children:"adaptive_weight()"})," and their\nsubroutines also respect this force confirm point during their checking.\nSpecifically, any fork before the force confirm height is ignored."]}),"\n",(0,i.jsxs)(n.p,{children:["Note that this force confirm rule is also defined based on ",(0,i.jsx)(n.em,{children:"past view"})," of each\nblock. With the computed anticone information, ",(0,i.jsx)(n.code,{children:"compute_timer_chain_tuple()"})," in\n",(0,i.jsx)(n.code,{children:"ConsensusGraphInner"})," computes the timer chain related information of each\nblock under its past view. The results of this function include the difference of\nthe ",(0,i.jsx)(n.code,{children:"timer_chain[]"}),", ",(0,i.jsx)(n.code,{children:"timer_chain_accumulative_lca[]"}),", and ",(0,i.jsx)(n.code,{children:"timer_chain_height"}),"\nbetween the ledger view and the past view. We can use the diff and the current\nledger view values to get the past view values."]}),"\n",(0,i.jsx)(n.h3,{id:"era",children:"Era"}),"\n",(0,i.jsxs)(n.p,{children:["In order to implement the checkpoint mechanism, the Conflux consensus algorithm split the\ngraph into eras. Every era contains ",(0,i.jsx)(n.code,{children:"era_epoch_count"})," epochs. For example, if the\n",(0,i.jsx)(n.code,{children:"era_epoch_count"})," is 50000, then there is a new era every 50000 epochs. The\npivot chain block at the height 50000 will be the genesis of a new era.\nAt the era boundary, there are several differences from the normal case."]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"A block will enter the total order for execution only if 1) it is under the\nsubtree of the previous era genesis and 2) it is inside the past set of the next era genesis in\nthe pivot chain."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Anticone penalty calculation for the block reward does not go across the era\nboundary."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"checkpoint",children:"Checkpoint"}),"\n",(0,i.jsxs)(n.p,{children:["Inside ",(0,i.jsx)(n.code,{children:"ConsensusGraphInner"}),", there are two key height pointers, the current\ncheckpoint era genesis block height (",(0,i.jsx)(n.code,{children:"cur_era_genesis_height"}),") and the current\nstable era genesis block height (",(0,i.jsx)(n.code,{children:"cur_era_stable_height"}),"). These two height pointers\nwill always point to some era genesis (being a multiple of ",(0,i.jsx)(n.code,{children:"era_epoch_count"}),").\nInitially, both of these two pointers will point to the true genesis (height\n0)."]}),"\n",(0,i.jsxs)(n.p,{children:["A new era genesis block becomes stable (i.e., ",(0,i.jsx)(n.code,{children:"cur_era_stable_height"})," moves) if\nthe block is ",(0,i.jsx)(n.em,{children:"force confirmed"})," in the current TreeGraph. A stable era genesis\nblock becomes a new checkpoint (i.e., ",(0,i.jsx)(n.code,{children:"cur_era_genesis_height"})," moves) if:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["The block is ",(0,i.jsx)(n.em,{children:"force confirmed in the past view of the stable era genesis block"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"In the anticone of this block, there is no timer chain block."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"should_move_stable_height()"})," and ",(0,i.jsx)(n.code,{children:"should_form_checkpoint_at()"})," in\n",(0,i.jsx)(n.code,{children:"ConsensusNewBlockHandler"})," are invoked after every newly inserted block to test\nthe above two conditions. Generally speaking, the stable era genesis block will never be\nreverted off the pivot chain. Any block in the past set of the checkpoint block\nis no longer required for the future computation of the consensus layer.\nTherefore, after a new checkpoint is formed, ",(0,i.jsx)(n.code,{children:"make_checkpoint_at()"})," in\n",(0,i.jsx)(n.code,{children:"ConsensusNewBlockHandler"})," is called to clean up those blocks that are not in\nthe future set of the new checkpoint."]}),"\n",(0,i.jsx)(n.p,{children:"Note that the checkpoint mechanism also changes how we handle a new block. For\na new block:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"If the new block is outside the subtree of the current checkpoint, we only\nneed to insert a stub into our data structure (because a block under the\nsubtree may be indirectly referenced via this stub block). We do not need to\ncare about such a block because it is not going to change the timer chain and it\nis not going to be executed."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["If the past set of the new block does not contain the stable era genesis block, we\ndo not need to check the partial invalid status of this block. This is because\nthis block will not change the timer chain (recall our assumption that the timer\nchain will not reorg for more than ",(0,i.jsx)(n.code,{children:"timer_chain_beta"})," blocks) and future blocks can reference\nthis block directly (since the timer chain difference is already more than ",(0,i.jsx)(n.code,{children:"timer_chain_beta"}),")."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"deferred-execution",children:"Deferred Execution"}),"\n",(0,i.jsxs)(n.p,{children:["Because the TreeGraph pivot chain may oscillate temporarily, we defer the\ntransaction execution for ",(0,i.jsx)(n.code,{children:"DEFERRED_STATE_EPOCH_COUNT"})," epochs (default 5).\nAfter a pivot chain update, ",(0,i.jsx)(n.code,{children:"activate_block()"})," routine will enqueue the\nexecution task of the new pivot chain except for the last five epochs. It calls\n",(0,i.jsx)(n.code,{children:"enqueue_epoch()"})," in ",(0,i.jsx)(n.code,{children:"ConsensusExecutor"})," to enqueue each task."]}),"\n",(0,i.jsx)(n.h3,{id:"block-reward-calculation",children:"Block Reward Calculation"}),"\n",(0,i.jsxs)(n.p,{children:["Because there is no explicit coinbase transaction in Conflux, all block rewards\nare computed implicitly during the transaction execution. In Conflux, the block\nreward is determined by the base reward and the penalty ratio based on the total weight of\nits anticone blocks divided by its epoch pivot block's target difficulty. This anticone set only\nconsiders blocks appearing no later than the next ",(0,i.jsx)(n.code,{children:"REWARD_EPOCH_COUNT"})," epochs.\nSpecifically, if there is a new era then the anticone set will not count across\nthe era boundary as well. ",(0,i.jsx)(n.code,{children:"get_pivot_reward_index()"})," in ",(0,i.jsx)(n.code,{children:"ConsensusExecutor"}),"\ncounts this reward anticone threshold.\n",(0,i.jsx)(n.code,{children:"get_reward_execution_info_from_index()"})," in ",(0,i.jsx)(n.code,{children:"ConsensusExecutor"})," and its\nsubroutines compute this anticone set given the threshold point in the pivot\nchain."]}),"\n",(0,i.jsx)(n.h3,{id:"blaming-mechanism",children:"Blaming Mechanism"}),"\n",(0,i.jsxs)(n.p,{children:["It is infeasible to validate the filled state root of a block because we\nwould need to execute all transactions in a different order in the past view of\nthat block. Instead, we will only ask full nodes to validate the state root\nresults on the current pivot chain. It then fills a blame number to indicate\nhow many levels ancestors from the parent who do not have correct state root.\nWhen this number is greater than zero, the filled deferred state root becomes a\nMerkel H256 vector that contains the corrected state roots of the ancestors\nalong with the correct one. ",(0,i.jsx)(n.code,{children:"get_blame_and_deferred_state_for_generation()"})," in\n",(0,i.jsx)(n.code,{children:"ConsensusGraph"})," computes the blame information for the block generation.\n",(0,i.jsx)(n.code,{children:"first_trusted_header_starting_from()"})," in ",(0,i.jsx)(n.code,{children:"ConsensusGraph"})," is a useful helper\nfunction to compute the first trustworthy header based on the subtree blame\ninformation."]}),"\n",(0,i.jsx)(n.h2,{id:"multi-thread-design",children:"Multi-Thread Design"}),"\n",(0,i.jsx)(n.p,{children:"The consensus layer has one thread dedicated to processing new blocks from the\nsynchronization layer and one thread dedicated to executing transactions. It of\ncourse also has a set of interface APIs that RPC threads and synchronization\nthreads may call."}),"\n",(0,i.jsx)(n.h3,{id:"consensus-worker",children:"Consensus Worker"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"Consensus Worker"})," is a thread created by the synchronization layer. During\nthe normal running phase, every new block will be sent to a channel connecting\nthe synchronization thread and the consensus worker thread. The consensus work\nthread consumes each block one by one and invokes ",(0,i.jsx)(n.code,{children:"consensus::on_new_block()"}),"\nto process it. Note that the synchronization layer ensures the new block to be\n",(0,i.jsx)(n.em,{children:"header-ready"})," when it is delivered to ",(0,i.jsx)(n.code,{children:"Consensus Worker"}),", i.e., all of its\nancestor/past blocks are already delivered to the consensus layer before itself.\nThis enables the consensus layer to always deal with a well-defined\ndirect acyclic graph without holes."]}),"\n",(0,i.jsxs)(n.p,{children:["One advantage of having a single thread to be dedicated to the consensus\nprotocol is that it simplifies the protocol implementation a lot. Because the\ndetails of the consensus protocol are complicated and the implementation involves\nmany sophisticated data structure manipulations, the single thread design makes\nsure that we do not need to worry about deadlocks or races. Upon the entrance\nof ",(0,i.jsx)(n.code,{children:"consensus::on_new_block()"}),", the thread acquires the write lock of the inner\nof the consensus struct (i.e., ConsensusGraphInner). During the normal phase,\nthis thread should be the only one modifying the inner struct of the consensus\nlayer."]}),"\n",(0,i.jsx)(n.h3,{id:"consensus-execution-worker",children:"Consensus Execution Worker"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"Consensus Execution Worker"})," is a thread created at the start of the consensus\nlayer. It is dedicated to transaction execution. There is a channel connecting\n",(0,i.jsx)(n.code,{children:"Consensus Worker"})," with ",(0,i.jsx)(n.code,{children:"Consensus Execution Worker"}),". Once the consensus\nprotocol determines the order of the pivot chain, it will send an ",(0,i.jsx)(n.code,{children:"ExecutionTask"}),"\nfor each epoch in the pivot chain to the channel. These tasks will be picked up\nby the ",(0,i.jsx)(n.code,{children:"Consensus Execution Worker"})," thread one by one. The thread loads the\nprevious state before the executed epoch from the storage layer as the input,\nruns all transactions in the executed epoch (see\n",(0,i.jsx)(n.code,{children:"ConsensusExecutor::process_epoch_transactions()"}),"), and produces the result\nstate as the output."]}),"\n",(0,i.jsxs)(n.p,{children:["The rationale of separating the transaction execution from the consensus\nprotocol implementation is for performance. With our ",(0,i.jsx)(n.em,{children:"blaming mechanism"}),", the\nexecution result state is completely separated from the consensus protocol\nimplementation. The ",(0,i.jsx)(n.em,{children:"deferred execution mechanism"})," gives us extra room to\npipeline the consensus protocol and the transaction execution. It is therefore\nnot wise to block the ",(0,i.jsx)(n.code,{children:"Consensus Worker"})," thread to wait for the execution\nresults from coming back."]}),"\n",(0,i.jsx)(n.h2,{id:"key-assumptions-invariants-and-rules",children:"Key Assumptions, Invariants, and Rules"}),"\n",(0,i.jsx)(n.p,{children:"If you want to write code to interact with the Conflux consensus layer, it is\nvery important to understand the following assumptions and rules."}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["The consensus layer assumes that the passed ",(0,i.jsx)(n.code,{children:"BlockDataManager"})," is in a\nconsistent state. It means that the ",(0,i.jsx)(n.code,{children:"BlockDataManager"})," contains the correct current\ncheckpoint/stable height. Blocks before the checkpoint and the stable height\nare properly checked during previous execution and they are persisted into the\n",(0,i.jsx)(n.code,{children:"BlockDataManager"})," properly. The consensus layer ",(0,i.jsx)(n.strong,{children:"does not check"})," the results\nit fetches from the block data manager. If it is inconsistent, the consensus\nlayer will execute incorrectly or crash!"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Besides the subroutines of ",(0,i.jsx)(n.code,{children:"on_new_block()"}),", ",(0,i.jsx)(n.strong,{children:"no one should hold the write\nlock of the inner struct"}),"! Right now the only exception for this rule is\n",(0,i.jsx)(n.code,{children:"assemble_new_block_impl()"})," because of computing the adaptive field and this is\nnot good we plan to change it. Acquiring the write lock of the inner struct\nis very likely to cause deadlock given the complexity of the Consensus layer\nand its dependency with many other components. Always try to avoid this!"]}),"\n"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,o.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>c,a:()=>r});var i=t(67294);const o={},s=i.createContext(o);function r(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);